<!DOCTYPE>
<html>
 <meta charset = "UTF-8">
 <meta name="viewport" content="width=device-width, initial-scale=1.0">
 <link  rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
      <head>
           <header>
    <img src="university of surrey logo.jpg" alt="Banner" align="center" width="500" height="120">        
    <h1 align="center"><b>ENG0018  Computer  Laboratory 2024/25</b></h1>
    <h2 align="center">Student URN: 6896024</h2>
      </head>
             </header>
 <body style="background-color:#E0E0E0; margin-left:100px; margin-right:100px; ">
 <style>
  header {
  padding: 20px;
  text-align: center;
  background: #E0E0E0;
  color: black;
  font-size: 30px;
}
  #main-title {
  padding-top: 100px;
  padding-bottom: 50px;
}
  body {
    background-color: #E0E0E0;
}
  table {
   margin: 20px auto;
   border-collapse: collapse;
   background-color: #f0f0f0;
   width: 90%;
   max-width: 600px;
}
  table {
   margin: 20px auto;
   border-collapse: collapse;
   background-color: #f0f0f0;
   width: 90%;
   max-width: 600px;
   text-align: center; /* This centers the content of the table */
}

th, td {
   text-align: center; /* Ensure that content inside table cells is centered */
}

h4 {
   text-align: center; /* Center-align the h4 text in the table rows */
   margin: 0;  /* Remove any unwanted spacing around the h4 */
}
  tr {
   border: 1px solid#000000;
    text-align: left;
   padding: 10px;
  }
  th{ 
   background-color: #808080
  }
  tr:nth-child(even){
  background-color: #D3D3D3
  }
  a {
  text-decoration: none;
}
 </style>
  <table id="customers" align="center"; >
      <tr>
        <th colspan="2">
          <h1>Table of Contents</h1>
        </th>
      </tr>
      <tr>
        <td><a href="#abstract">Abstract</a></td>
      </tr>
      <tr>
        <td><a href="#introduction">Introduction</a></td>
      </tr>
      <tr>
        <td><a href="#lyricalgeneration">Lyrical Generation</a></td>
      </tr>
      <tr>
        <td><a href="#vocalgeneration">Vocal Generation</a></td>
      </tr>
      <tr>
        <td><a href="#musicgenerating">Generating the Music itself</a></td>
      </tr>
      <tr>
        <td><a href="#legal">Legality of AI-generated music</a></td>
      </tr>
      <tr>
        <td><a href="#conclusion">Conclusion</a></td>
      </tr>
      <tr>
        <td><a href="#references">References</a></td>
      </tr>
    </table>
<h1 id="main-title" align="center"; ><b>How AI is implemented to create commercial music from Scratch</b></h1>
 <h2 id="Abstract"><b>Abstract</b></h2>
  <p>
   This page explores the role of generative AI in music creation focusing on key components such as Lyric Generation, vocal synthesis and audio/wave synthesis. We explore how chat GPT works to generate lyrical content and how technologies like Tacotron 2 and WaveNet are used to generate vocals and audio through waveform synthesis. We will also explore the legality of using this technology for commercial music purposes and whether this violates copyright laws or the right of publicity. 
  </p>
  <h3 id="Introduction"><b>Introduction</b></h3>
  <p class="ex1">
    Since the release of Chat GPT by open AI in 2022, artificial intelligence has been the hottest topic when it comes to technology, and has been rapidly implemented into several different industries, an example of which - the music industry. From generating fully recorded, mixed and mastered songs of any genre, to replicating the vocals of reputable artists, let's break down exactly how generative AI is used to create music, and explore the potential uses and legal concerns of this technology. The current and most popular AI music generators on the market are Soundraw, Soundful, Suno.ai and Mubert . Like in Figure 1, these AI’s generate music from a text input from the user, where the user describes the genre, lyrical theme and tempo of the song they desire.  Generating a song requires 3 main components, Lyrical Generation, Vocal Generation and generating the soundtrack itself, which Suno manages to do all in one go. But let’s look at each process in detail
  </p>
   <img src="fig1.png"  alt="Figure 1" style="width: 1000px; height: 200px; align="center";> 
    <h3 id="lyricalgeneration"><b>Lyrical Generation</b></h3>
  <p>
   Most of these AIs use their own custom built versions of Chat GPT. GPT is a Generative Pretrained Transformer model (Harry Guinness, 2024), which is a language model trained across the large database of the internet, and any other publicly available datasets or pieces of information. After analyzing several lyrics from the internet, Chat GPT would consider the grammar, semantics, context, style and genre and generate lyrics that would then be inputted into the Speech/Audio Generator.
First, the user’s sentence/prompt is broken down into smaller chunks called ‘tokens’, which are chunks of text encoded as a vector as in Figure 2. ChatGPT uses a transformer model to read every word in the sentence at once, and compares each word/token to the other by giving each word an “attention score” from 0 to 1.  Words with a similar attention score, typically correlate in some way. Attention mechanisms help the model focus on certain words and understand the holistic subject of the sentence, for example it would be able to understand the meaning of the word “bank” based off of the surrounding text, either like a ‘river bank’ or a ‘financial bank’. Once a full set of lyrics is generated, the model outputs the final sequence of tokens as text, which could then be passed onto a Speech/Vocal Generator to turn the text into audio.
  </p>
   <img src="fig2.png" alt="Figure 2" style="width: 350px; height: 250px; align="right"; >
  <br>
      <h3 id="vocalgeneration"><b>Vocal Generation</b></h3>
  <p>
  Once the lyrics of the song are generated through a Large Language Model or similar, AI voice cloning is used to replicate the vocals of a chosen artist and a text-to-speech model would be used to bring those lyrics to life. AI Vocal generators use models similar to that of one called Tacotron 2 - a Text-to-Speech model that converts text into natural sounding speech, to generate vocals from a text input, so let’s break down how Tacotron 2 works. 
   <br> 
   <ol class="numbered-list">
  <li><b>Text Processing - </b>Firstly, the text undergoes several stages of preprocessing to ensure that it is in a format that the model can actually handle. Tacotron cleans up the lyrical text input, removing any punctuation marks, turning number inputs like “123” into “one two three” and abbreviations like “Dr.” to “Doctor”.</li>
  <li><b>Grapheme-to-Phoneme (G2P) Conversion - </b> 
Text is usually written in “Graphemes” which are letters or characters like “cat”, “dog” and etc as seen in Figure 4. Speech is made up of “Phonemes”, which are the smallest units of sound in speech and are written as “/k/, /æ/, /t/”. Tacotron would then use a ‘Grapheme-to-Phoneme’ model to convert the words into their phonetic representation like this "cat" → /k/, /æ/, /t/.
</li>
  <li><b>Encoding the text - </b>The phonemes would then be passed through an encoder which turns them into a list of numbers (vector) that summarises their tone, stress and rhythm.
</li>
    <li><b>Mel-Spectrogram Generation - </b> This vector would then be put into a decoder which generates a spectrogram (frequency:time graph) from the vector.
</li>
    <li><b>Waveform Synthesis - </b> The Mel Spectrogram would be passed down to a Vocoder like WaveNet, which would analyse the spectrogram and reconstruct it into actual audio. WaveNet is a machine learning model that learns from hours of audio data, and it would ‘predict’ how the text would sound based on the data it has analysed and the spectrogram. </li>
</ol>

  </p>
  <br>
      <h3 id="musicgenerating"><b>Generating the Music itself</b></h3>
  <p>
There are many platforms such as Mureka.ai and Suno.ai that can be used to generate songs from scratch, given some prompts on genre, topic, tempo and lyrics from the user. But how exactly do these platforms generate the soundtrack ? <br> Just like in generating the AI vocal and lyrics, music generating AIs start by scraping their data from publicly available datasets and online repositories and user contributed data. This includes audio files, MIDI files and ABC Notation. An example of such datasets is ‘MusicCaps’ which is a dataset containing 5,521 music samples of which is labeled with an English aspect list and a free text caption written by musicians. The instrument choice, tempo, meter and key are provided by the user. WaveNet uses convolutional neural networks  to make predictions influenced by all previously seen observations of the waveforms it has analysed and generates new waveforms that are iterations of the previous (Rachel Chen, 2017). 
  </p>
   <img src="fig3.png"  alt="Figure 3" style="width: 400px; height: 300px; align:right; margin-top: -20px;" >
  <br>
     <h3 id="legal"><b>Legality of AI generated music</b></h3>
  <p>
Copyright law actually doesn’t directly protect a singer's voice, but rather the Right of Publicity which is the right to control the commercial use of their name, likeness, or voice. This means that the artist has the right to claim royalties on the song and even have it taken down (The IP Press, 2024).
  </p>
  <br>  
    <h2 id="conclusion"><b>Conclusion</b></h2>
    <p>
     We’re still a long way away from fully replacing singers and musicians. A lot of these models create music that sounds generated, and lacking human feel, but as the internet grows, AI models will acquire more data to train on and become better.
     </p>
    <br>
    <h2 id="references"><b>Referrences</b></h2>
   <ul>
    <li> Guiness , H (2024) <a href="https://zapier.com/blog/how-does-chatgpt-work/"><i> How does ChatGPT work? </i></a>
     <p>Available at : https://zapier.com/blog/how-does-chatgpt-work/</p>
    <p>
     (Accessed date: 24 November 2024)
    </p>
    </li>
    <li> William, S (2021) <a href="https://medium.com/@swilliam.productions/text-to-speech-with-tacotron-2-573986c42124"><i>Text To Speech With Tacotron 2:, Medium.com</i></a>
    <br>
     <p> Available at : https://medium.com/@swilliam.productions/text-to-speech-with-tacotron-2-573986c42124 </p>
     <p>
       (Accessed date: 27 November 2024)
     </p>
    </li>
    <li>  <i> AI voice cloning through the lens of copyright laws: challenges on the rights of singers </i> (2024) <br> <a href="https://www.theippress.com/2024/06/09/ai-voice-cloning-through-the-lens-of-copyright-laws-challenges-on-the-rights-of-singers/#:~:text=Protection%20of%20artists%20voice%20under,qua%20non%20of%20copyright%20protection"> 
     <br> Available at: https://www.theippress.com/2024/06/09/ai-voice-cloning-through-the-lens-of-copyright-laws-challenges-on-the-rights-of-singers/#:~:text=Protection%20of%20artists%20voice%20under,qua%20non%20of%20copyright%20protection
     <br> 
     <br>
     (Accessed date: 21 November 2024)
     <br>
     <br>
 </a></li>
    <li> Chen, R (2017) <a href="https://medium.com/@rachelchen_49210/generating-ambient-noise-from-wavenet-95aa7f0a8f77"><i>Generating Ambient Music from WaveNet, Medium.com</i></a>
     <br> 
     Available at: https://medium.com/@rachelchen_49210/generating-ambient-noise-from-wavenet-95aa7f0a8f77 
     <br>
    (Accessed date: 23 November 2024)
    </li>
</ul>
    <br>
</p>
    <br>
 </body>
</html>
