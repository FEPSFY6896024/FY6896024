<!DOCTYPE html>
<html lang="en">
 <meta charset = "UTF-8">
 <meta name="viewport" content="width=device-width, initial-scale=1.0">
 <link  rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
      <head>


 <style>
  header {
  padding: 20px;
  text-align: center;
  background: #E0E0E0;
  color: black;
  font-size: 30px;
}
  #main-title {
  padding-top: 100px;
  padding-bottom: 100px;
}
  body {
    background-color: #E0E0E0;
   padding: 20px;
   text-align: justify;
}
  video {
   align: center;
  }

  table {
   margin: 20px auto;
   border-collapse: collapse;
   background-color: #f0f0f0;
   width: 90%;
   max-width: 600px;
   text-align: center; /* This centers the content of the table */
}

th, td {
   text-align: center; /* Ensure that content inside table cells is centered */
}

h4 {
   text-align: center; /* Center-align the h4 text in the table rows */
   margin: 0;  /* Remove any unwanted spacing around the h4 */
}
  tr {
   border: 1px solid#000000;
    text-align: left;
   padding: 10px;
  }
  th{ 
   background-color: #808080
  }
  tr:nth-child(even){
  background-color: #D3D3D3
  }
  a {
  text-decoration: none;
}
 </style>
                  <header>
    <img src="university of surrey logo.jpg" alt="Banner" align="center" width="500" height="120">        
    <h1 align="center"><b>ENG0018  Computer  Laboratory 2024/25</b></h1>
    <h2 align="center">Student URN: 6896024</h2>
      </head>
             </header>
  <table id="customers" align="center"; >
      <tr>
        <th colspan="2">
          <h1>Table of Contents</h1>
        </th>
      </tr>
      <tr>
        <td><a href="#Abstract">Abstract</a></td>
      </tr>
      <tr>
        <td><a href="#Introduction">Introduction</a></td>
      </tr>
      <tr>
        <td><a href="#lyricalgeneration">Lyrical Generation</a></td>
      </tr>
      <tr>
        <td><a href="#vocalgeneration">Vocal Generation</a></td>
      </tr>
      <tr>
        <td><a href="#musicgenerating">Generating the Music itself</a></td>
      </tr>
      <tr>
        <td><a href="#legal">Legality of AI-generated music</a></td>
      </tr>
      <tr>
        <td><a href="#conclusion">Conclusion</a></td>
      </tr>
      <tr>
        <td><a href="#references">References</a></td>
      </tr>
    </table>
<h1 id="main-title" align="center"; ><b>How AI is implemented to create commercial music from Scratch</b></h1>
 <h2 id="Abstract"><b>Abstract</b></h2>
  <p>
   This page explores the role of generative AI in music creation focusing on key components such as Lyric Generation, vocal synthesis and audio/wave synthesis. We explore how chat GPT works to generate lyrical content and how technologies like Tacotron 2 and WaveNet are used to generate vocals and audio through waveform synthesis. We will also explore the legality of using this technology for commercial music purposes and whether this violates copyright laws or the right of publicity. 
  </p>
  <h3 id="Introduction"><b>Introduction</b></h3>
  <p id="Introduction_InText" class="ex1">
    Since the release of Chat GPT by open AI in 2022, artificial intelligence has been the hottest topic when it comes to technology, and has been rapidly implemented into several different industries, an example of which - the music industry. From generating fully recorded, mixed and mastered songs of any genre, to replicating the vocals of reputable artists, let's break down exactly how generative AI is used to create music, and explore the potential uses and legal concerns of this technology. The current and most popular AI music generators on the market are Soundraw, Soundful, Suno.ai and Mubert . Like in Figure 1, these AI’s generate music from a text input from the user, where the user describes the genre, lyrical theme and tempo of the song they desire.  Generating a song requires 3 main components, Lyrical Generation, Vocal Generation and generating the soundtrack itself, which Suno manages to do all in one go. But let’s look at each process in detail
  </p>
   <img src="fig1.png" alt="Figure 1" style="width: 900px; height: 200px; display: block; margin-left: auto; margin-bottom: 30px; margin-right: auto;"> 
    <h3 id="lyricalgeneration"><b>Lyrical Generation</b></h3>
    <div style="text-align: right; margin-top: 10px; margin-bottom: 20px; float: right;">
    <img src="fig2.png" alt="Figure 2" style="width: 500px; height: 350px; float: right; margin-left: 20px; margin-bottom:20px; margin-top:10px"> 
    </div>
  <p id="LyricalGeneration2">
   Most of these AIs use their own custom built versions of Chat GPT. GPT is a Generative Pretrained Transformer model (Harry Guinness, 2024), which is a language model trained across the large database of the internet, and any other publicly available datasets or pieces of information. After analyzing several lyrics from the internet, Chat GPT would consider the grammar, semantics, context, style and genre and generate lyrics that would then be inputted into the Speech/Audio Generator.
First, the user’s sentence/prompt is broken down into smaller chunks called ‘tokens’, which are chunks of text encoded as a vector as in Figure 2. ChatGPT uses a transformer model to read every word in the sentence at once, and compares each word/token to the other by giving each word an “attention score” from 0 to 1.  Words with a similar attention score, typically correlate in some way. Attention mechanisms help the model focus on certain words and understand the holistic subject of the sentence, for example it would be able to understand the meaning of the word “bank” based off of the surrounding text, either like a ‘river bank’ or a ‘financial bank’. Once a full set of lyrics is generated, the model outputs the final sequence of tokens as text, which could then be passed onto a Speech/Vocal Generator to turn the text into audio.
  </p>
  <br>
      <h3 id="vocalgeneration"><b>Vocal Generation</b></h3>
  <p id="Voicecloningintro">
  Once the lyrics of the song are generated through a Large Language Model or similar, AI voice cloning is used to replicate the vocals of a chosen artist and a text-to-speech model would be used to bring those lyrics to life. AI Vocal generators use models similar to that of one called Tacotron 2 - a Text-to-Speech model that converts text into natural sounding speech, to generate vocals from a text input, so let’s break down how Tacotron 2 works. 
   <br> 
   <ol class="numbered-list" id="Tacotron">
  <li><b>Text Processing - </b>Firstly, the text undergoes several stages of preprocessing to ensure that it is in a format that the model can actually handle. Tacotron cleans up the lyrical text input, removing any punctuation marks, turning number inputs like “123” into “one two three” and abbreviations like “Dr.” to “Doctor”.</li>
    <img src="fig4.png" alt="Figure 4" style="width: 400px; height: 200px; float: right; margin-left: 20px; margin-bottom:20px; margin-top:10px"> 
    <li><b>Grapheme-to-Phoneme (G2P) Conversion - </b> 
Text is usually written in “Graphemes” which are letters or characters like “cat”, “dog” and etc as seen in Figure 4. Speech is made up of “Phonemes”, which are the smallest units of sound in speech and are written as “/k/, /æ/, /t/”. Tacotron would then use a ‘Grapheme-to-Phoneme’ model to convert the words into their phonetic representation like this "cat" → /k/, /æ/, /t/.
</li>
  <li><b>Encoding the text - </b>The phonemes would then be passed through an encoder which turns them into a list of numbers (vector) that summarises their tone, stress and rhythm.
</li>
    <li><b>Mel-Spectrogram Generation - </b> This vector would then be put into a decoder which generates a spectrogram (frequency:time graph) from the vector.
</li>
    <li><b>Waveform Synthesis - </b> The Mel Spectrogram would be passed down to a Vocoder like WaveNet, which would analyse the spectrogram and reconstruct it into actual audio. WaveNet is a machine learning model that learns from hours of audio data, and it would ‘predict’ how the text would sound based on the data it has analysed and the spectrogram. </li>
</ol>
  </p>
  <br>
      <h3 id="musicgenerating"><b>Generating the Music itself</b></h3>
    <div style="text-align: right; margin-top: 10px; margin-bottom: 20px; float: right;">
      <img src="fig3.png" alt="Figure 3" style="width: 300px; height: 250px; float: right; margin-left: 20px; margin-bottom:20px; margin-top:10px"> 
     </div>
  <p id="MusicGeneration">
There are many platforms such as Mureka.ai and Suno.ai that can be used to generate songs from scratch, given some prompts on genre, topic, tempo and lyrics from the user. But how exactly do these platforms generate the soundtrack ? <br> Just like in generating the AI vocal and lyrics, music generating AIs start by scraping their data from publicly available datasets and online repositories and user contributed data. This includes audio files, MIDI files and ABC Notation. An example of such datasets is ‘MusicCaps’ which is a dataset containing 5,521 music samples of which is labeled with an English aspect list and a free text caption written by musicians. The instrument choice, tempo, meter and key are provided by the user. WaveNet uses convolutional neural networks  to make predictions influenced by all previously seen observations of the waveforms it has analysed and generates new waveforms that are iterations of the previous (Rachel Chen, 2017). 
  </p>
  <br>
     <h3 id="legal"><b>Legality of AI generated music</b></h3>
 <section id="legality2">
  <p>
Copyright law actually doesn’t directly protect a singer's voice, but rather the Right of Publicity which is the right to control the commercial use of their name, likeness, or voice. This means that the artist has the right to claim royalties on the song and even have it taken down (The IP Press, 2024).
  </p>
 </section>
  <br>  
    <h2 id="conclusion"><b>Conclusion</b></h2>
    <p = "theconclusion">
     We’re still a long way away from fully replacing singers and musicians. A lot of these models create music that sounds generated, and lacking human feel, but as the internet grows, AI models will acquire more data to train on and become better.
     </p>
    <br>
    <h2 id="references"><b>Referrences</b></h2>
   <ul>
    <li> Guiness , H. (2024) <a href="https://zapier.com/blog/how-does-chatgpt-work/"><i> How does ChatGPT work?. </i></a>
     <p>Available at : https://zapier.com/blog/how-does-chatgpt-work/</p>
    <p>
     (Accessed date: 24 November 2024).
    </p>
    </li>
    <li> William, S. (2021) <a href="https://medium.com/@swilliam.productions/text-to-speech-with-tacotron-2-573986c42124"><i>Text To Speech With Tacotron 2:.</i></a>
    <br>
     <p> Available at : https://medium.com/@swilliam.productions/text-to-speech-with-tacotron-2-573986c42124 </p>
     <p>
       (Accessed date: 27 November 2024).
     </p>
    </li>
    <li>  <i> AI voice cloning through the lens of copyright laws: challenges on the rights of singers. </i> (2024) <br> <a href="https://www.theippress.com/2024/06/09/ai-voice-cloning-through-the-lens-of-copyright-laws-challenges-on-the-rights-of-singers/#:~:text=Protection%20of%20artists%20voice%20under,qua%20non%20of%20copyright%20protection"> 
     <br> Available at: 
     <br>https://www.theippress.com/2024/06/09/ai-voice-cloning-through-the-lens-of-copyright-laws-challenges-on-the-rights-of-singers/#:~:text=Protection%20of%20artists%20voice%20under,qua%20non%20of%20copyright%20protection
     <br> 
     <br>
     (Accessed date: 21 November 2024.
     <br>
     <br>
 </a></li>
    <li> Chen, R. (2017) <a href="https://medium.com/@rachelchen_49210/generating-ambient-noise-from-wavenet-95aa7f0a8f77"><i>Generating Ambient Music from WaveNet.</i></a>
     <br> 
     <br>
     Available at: https://medium.com/@rachelchen_49210/generating-ambient-noise-from-wavenet-95aa7f0a8f77 
     <br>
     <br>
    (Accessed date: 23 November 2024).
     <br>
    </li>
</ul>
</p>
 <h3><b><u>Presentation Video</u></b></h3>
   <video src="ENG0018 Presentation.mp4" align="center" width="800" height="450" controls>
  </video>
 <!-- //////////////////////////////////////////////////////////////////////////////// -->
  <!-- ////////////////////////////// Adding last update ////////////////////////////// -->
  <!-- //////////////////////////////////////////////////////////////////////////////// -->
    <!-- Last commit time display -->
<div id="last-updated">Loading last update time...</div>
<!-- Verify Button -->
<button onclick="verifyLastUpdatedTime()" style="display: block; margin: 10px auto; padding: 8px 16px;">
    Verify Last Modified Time
</button>
<script>
    async function getLastUpdatedTime() {
        const username = 'FEPSFY6896024';
        const repo = 'FY6896024';
       
        const url = `https://api.github.com/repos/${username}/${repo}/commits`;
        try {
            const response = await fetch(url, {
                method: 'GET',
                headers: {
                    'Accept': 'application/vnd.github.v3+json',
                }
            });
            if (!response.ok) {
                throw new Error(`Error fetching data: ${response.status} - ${response.statusText}`);
            }
            const commits = await response.json();
            if (commits && commits.length > 0) {
                const lastCommitDate = new Date(commits[0].commit.committer.date);
               
                // Displaying the time on load
                document.getElementById('last-updated').innerText = `Last Modified Time: ${lastCommitDate.toLocaleString()}`;
            } else {
                document.getElementById('last-updated').innerText = 'No commits found in the repository.';
            }
        } catch (error) {
            console.error('Error fetching the last updated time:', error);
            document.getElementById('last-updated').innerText = 'Error fetching update time. Please check the repository details.';
        }
    }
    // Function to verify the last update time by re-fetching it from the API
    async function verifyLastUpdatedTime() {
        document.getElementById('last-updated').innerText = 'Verifying...';
        await getLastUpdatedTime();
        alert("Last modified time has been successfully verified from GitHub API.");
    }
    // Initial load to display the time on page load
    window.onload = getLastUpdatedTime;
</script>
 
 <!-- Placeholder for total word count -->
<p id="totalWordCount"></p>
<hr>
<script>
  // Function to calculate and display word count for a specified section
  function displayWordCount(sectionId, outputId) {
    // Get the text content from the specified section
    const text = document.getElementById(sectionId).textContent;
    // Split text into words based on spaces and filter out any empty strings
    const wordArray = text.trim().split(/\s+/);
    // Count the words
    const wordCount = wordArray.length;
    // Return the word count for summing purposes
    return wordCount;
  }
  // Function to calculate and display total word count from selected sections
  function displayTotalWordCount() {
    // Calculate word count for each section and accumulate the total
    const Introduction_InTextCount = displayWordCount("Introduction_InText");
    const LyricalGeneration2Count = displayWordCount("LyricalGeneration2");
    const VoicecloningintroCount = displayWordCount("Voicecloningintro");
    const TacotronCount = displayWordCount("Tacotron");
    const MusicGenerationCount = displayWordCount("MusicGeneration");
    const legality2Count = displayWordCount("legality2");
    const theconclusionCount = displayWordCount("theconclusion"); 
    // Calculate the sum of all selected sections
    const totalWordCount = Introduction_InTextCount + LyricalGeneration2Count + VoicecloningintroCount + TacotronCount + MusicGenerationCount + legality2Count  theconclusionCount;
    // Display the total word count
    document.getElementById("totalWordCount").innerText = `Total word count: ${totalWordCount}`;
  }
  // Run the function for specific sections and display total count when the page loads
  window.onload = displayTotalWordCount;
</script>
   </p>
 </body>
</html>


